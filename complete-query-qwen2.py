import re
import json
import ollama
from datetime import datetime

paper_list = [
	"Affective Learning Objectives for Communicative Visualizations.##When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents\u2013those that seek to influence or leverage the audience's opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived \u2018neutrality\u2019 or are \u2018political,\u2019 designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions\u2013such as advocacy visualizations or persuasive cartography\u2013we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.",
	"Picture Fuzzy Interactional Bonferroni Mean Operators via Strict Triangular Norms and Applications to Multicriteria Decision Making.##Based on the closed operational laws in picture fuzzy numbers and strict triangular norms, we extend the Bonferroni mean (BM) operator under the picture fuzzy environment to propose the picture fuzzy interactional Bonferroni mean (PFIBM), picture fuzzy interactional weighted Bonferroni mean (PFIWBM), and picture fuzzy interactional normalized weighted Bonferroni mean (PFINWBM) operators. We prove the monotonicity, idempotency, boundedness, and commutativity for the PFIBM and PFINWBM operators. We also establish a novel multicriteria decision making (MCDM) method under the picture fuzzy environment by applying the PFINWBM operator. Furthermore, we apply our MCDM method to the enterprise resource planning (ERP) systems selection. The comparative results for our MCDM method induced by six classes of well-known triangular norms ensure that the best selection is always the same ERP system. Therefore, our MCDM method is effective for dealing with the picture fuzzy MCDM problems.",
	"Inferring Affective Experience from the Big Picture Metaphor_ A Two-dimensional Visual Breadth Model.##This study explores the psychological significance of the commonly used visual metaphor'seeing the big picture'and examines whether and how it leads to positive experiences in real-life situations. To elucidate this phenomenon, a two-dimensional model of visual breadth is proposed, then respectively operationalized by two computer vision approaches. Our approaches are evaluated on a collected data set with 29,216 photos. The results revealed that physical and contextual breadth are two essential visual structures that compose a'big picture'. Furthermore, these two visual breadths interactively shape people's affective experiences. This study provides insight into the psychological implications of the'big picture'metaphor and sheds light on its practical potential for computer vision approaches and affective computing in the wild.",
	"Modelling Stochastic Context of Audio-Visual Expressive Behaviour With Affective Processes.##Recognising apparent emotion from audio-visual signals in naturalistic conditions remains an open problem. Existing methods that build on recurrent models, or in the modelling of contextual dependencies at the feature level using self-attention fail to model the long-term dependencies that subtly occur at different levels of abstraction. Affective Processes have emerged as a novel paradigm to the modelling of temporal dynamics through a probabilistic global latent variable that captures context and induces dependencies in the outputs, showing superior performance with little complexity. Despite its impressive results on visual data, Affective Processes remain unexplored in the domain of audio data, known to crucially influence the perception of emotions. In this paper, we first revisit and extend Affective Processes to the speech domain, identifying the key components and learning procedures for their efficient training. We then extend Affective Processes to audio-visual affect recognition, using modality-specific context encoders. Finally, we propose a novel application of Affective Processes in the domain of Cooperative Machine Learning for propagating affect labels in videos using sparse human supervision. We conduct extensive ablation studies, identifying the main components behind the success of Affective Processes, as well as comparisons against existing works in a variety of datasets.",
	"ConspEmoLLM_ Conspiracy Theory Detection Using an Emotion-Based Large Language Model.##The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (ie, sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related discussion (eg, opinions towards theories). ConspEmoLLM is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset, which includes five tasks to support LLM instruction tuning and evaluation. We demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms several open-source general domain LLMs and ChatGPT, as well as an LLM that has been fine-tuned using ConDID, but which does not use affective features. ConspEmoLLM can be easily applied to identify and classify conspiracy-related text in the real world. The work has been released at unmapped: uri https://github. com/lzw108/ConspEmoLLM/.",
	"Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence.##Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a \"small yet powerful\" physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at https://remap-dataset.github.io/ReMAP."
]

for p in paper_list:
	query = f"'''{p}'''\n请你分析文章的研究领域和研究内容:"
	now = datetime.now()
	filename = "query-in-qwen2-" + now.strftime("%Y%m%d_%H%M%S")
	with open(f'log-qwen2/{filename}.json', 'w', encoding='utf-8') as result_log:
		response = ollama.chat(model='qwen2.5:72b', messages=[
			{
			'role': 'user',
			'content': query,
			},
		])
		print(query)
		resp = re.sub(r'\n\n', '\n', response['message']['content'])
		data = {
			'query': query,
			'response': resp
		}
		json.dump(data, result_log, indent=4, ensure_ascii=False)